{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18656a82",
   "metadata": {},
   "source": [
    "# install required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf68cfa",
   "metadata": {},
   "source": [
    "## NOTE: This environment has been installed basic libraries like as torch, jupyter, pandas, numpy, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85dc792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opendatasets transformers pandas-profiling -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec91fdf",
   "metadata": {},
   "source": [
    "# prepare data files and authorize kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32e22ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: lamttic\n",
      "Your Kaggle Key: ········\n",
      "Downloading nlp-getting-started.zip to ./nlp-getting-started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 593k/593k [00:00<00:00, 120MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./nlp-getting-started/nlp-getting-started.zip to ./nlp-getting-started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download('https://www.kaggle.com/c/nlp-getting-started', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fcb48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./nlp-getting-started/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224fde0",
   "metadata": {},
   "source": [
    "# load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01abf8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       0\n",
       "2         3       0\n",
       "3         9       0\n",
       "4        11       0\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       0\n",
       "3260  10868       0\n",
       "3261  10874       0\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sample_df = pd.read_csv('sample_submission.csv')\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b817ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f443b55e050>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bc209",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46eea19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f39d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1f6f91dc404c3582af9a49cf123a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf70c06bbeb4626bd9da5a74ac723b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7314a35dc4da4a90ab3a4eb50de7eab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbe67d8172b431f81ab5522b1d82b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0395f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df['text'].values\n",
    "train_labels = train_df['target'].values\n",
    "test_sentences = test_df['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c22b1d",
   "metadata": {},
   "source": [
    "## show frequency of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9fe51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  85.,  296.,  445.,  551.,  779.,  973.,  961., 1032., 2334.,\n",
       "         157.]),\n",
       " array([  7.,  22.,  37.,  52.,  67.,  82.,  97., 112., 127., 142., 157.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([len(sentence) for sentence in train_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ce9972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 35., 111., 153., 218., 272., 408., 388., 386., 755., 537.]),\n",
       " array([  5. ,  19.6,  34.2,  48.8,  63.4,  78. ,  92.6, 107.2, 121.8,\n",
       "        136.4, 151. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(sentence) for sentence in test_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d231dc",
   "metadata": {},
   "source": [
    "# tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7218e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(sentence) for sentence in np.concatenate([train_sentences, test_sentences], axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36dcd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encode_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        attention_masks.append(encode_dict['attention_mask'])\n",
    "        \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d92f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks = encode_sentences(train_sentences, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a1c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f9a20",
   "metadata": {},
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5eb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe2ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a1503",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd04e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel\n",
    "\n",
    "class CustomBert(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        config.output_hidden_states = True\n",
    "        super(CustomBert, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = config.num_hidden_layers + 1\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        hidden_layers = outputs[2]\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer[:, 0, :]) for layer in hidden_layers], dim=2\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0) * cls_outputs).sum(-1)\n",
    "\n",
    "        # multisample dropout (wut): https://arxiv.org/abs/1905.09788\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [self.classifier(self.high_dropout(cls_output)) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        outputs = logits\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f99166a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46882a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/4101616258.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                            attention_mask=attention_mask)\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1121\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "models = []\n",
    "\n",
    "for fold, (train_dataset_ids, val_dataset_ids) in enumerate(kfold.split(dataset)):\n",
    "    ## set dataset and dataloader for train\n",
    "    batch_size = 16\n",
    "    \n",
    "    train_subsampler = SubsetRandomSampler(train_dataset_ids)\n",
    "    val_subsampler = SubsetRandomSampler(val_dataset_ids)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset,\n",
    "                                  sampler=train_subsampler,\n",
    "                                  batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(dataset,\n",
    "                                sampler=val_subsampler,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    # set pretrained model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CustomBert.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    # set optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    # set learning rate scheduler\n",
    "    epochs = 1\n",
    "    total_step = len(train_dataset_ids) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_step\n",
    "    )\n",
    "\n",
    "    def get_accuracy(preds, labels):\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(preds_flat == labels_flat) / len(preds_flat)\n",
    "\n",
    "    # fine-tuning model\n",
    "\n",
    "    creterion = nn.CrossEntropyLoss()\n",
    "    current_val_loss = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_val_loss = 0.0\n",
    "        total_val_accuracy = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader, 1):\n",
    "            input_ids, attention_mask, labels = tuple(el.to(device) for el in batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=attention_mask)\n",
    "\n",
    "            loss = creterion(output, labels)\n",
    "            total_train_loss += loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f'Train loss: {total_train_loss / len(train_dataloader)}')\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = tuple(el.to(device) for el in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids,\n",
    "                               token_type_ids=None,\n",
    "                               attention_mask=attention_mask)\n",
    "\n",
    "                loss = creterion(output, labels)\n",
    "                total_val_loss += loss\n",
    "\n",
    "\n",
    "            logits = output.detach().cpu().numpy()\n",
    "            label_ids = labels.detach().cpu().numpy()\n",
    "\n",
    "            total_val_accuracy += get_accuracy(logits, label_ids)\n",
    "\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f'Validation loss: {val_loss}')\n",
    "        print(f'Validation accuracy: {total_val_accuracy / len(val_dataloader)}')\n",
    "\n",
    "        if epoch == 0:\n",
    "            current_val_loss = val_loss\n",
    "        else:\n",
    "            if current_val_loss <= val_loss:\n",
    "                print(f'Early stop: {epoch} epoch')\n",
    "                break\n",
    "                \n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0da237",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "479f8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00721606",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids, test_attention_masks = encode_sentences(test_sentences, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0e9184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = torch.tensor(np.ones(len(test_input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "806bc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8422ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids, attention_mask, labels = tuple(el.to(device) for el in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        models_logits = []\n",
    "        \n",
    "        for model in models:\n",
    "            output = model(input_ids,\n",
    "                           attention_mask=attention_mask,\n",
    "                           token_type_ids=None)\n",
    "\n",
    "            logits = output.detach().cpu().numpy()\n",
    "            models_logits.append(logits)\n",
    "        \n",
    "        sum_logits = reduce(lambda x,y: x + y, models_logits)\n",
    "        pred_flatten = np.argmax(sum_logits, axis=1).flatten()\n",
    "\n",
    "    predictions.extend(pred_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731856f",
   "metadata": {},
   "source": [
    "# calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dd08de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4781d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d21a9762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8504443763407907"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_df[correct_df['target'].values == predictions].shape[0] / correct_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b074afd",
   "metadata": {},
   "source": [
    "# save submission file and submit predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7381709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c0515f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answer_submission.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'target'])\n",
    "    for idx, target in zip(test_df['id'].values, predictions):\n",
    "        writer.writerow([idx, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1730f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 25.4k/25.4k [00:06<00:00, 4.32kB/s]\n",
      "Successfully submitted to Natural Language Processing with Disaster Tweets"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f answer_submission.csv -m \"baseline + oof + multi_sample_dropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4426a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
