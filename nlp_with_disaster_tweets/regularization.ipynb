{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18656a82",
   "metadata": {},
   "source": [
    "# install required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf68cfa",
   "metadata": {},
   "source": [
    "## NOTE: This environment has been installed basic libraries like as torch, jupyter, pandas, numpy, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85dc792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opendatasets transformers pandas-profiling -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec91fdf",
   "metadata": {},
   "source": [
    "# prepare data files and authorize kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e22ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: lamttic\n",
      "Your Kaggle Key: ········\n",
      "Downloading nlp-getting-started.zip to ./nlp-getting-started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 593k/593k [00:00<00:00, 51.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./nlp-getting-started/nlp-getting-started.zip to ./nlp-getting-started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download('https://www.kaggle.com/c/nlp-getting-started', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fcb48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./nlp-getting-started/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224fde0",
   "metadata": {},
   "source": [
    "# load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sample_df = pd.read_csv('sample_submission.csv')\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b817ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bc209",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eea19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f39d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0395f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df['text'].values\n",
    "train_labels = train_df['target'].values\n",
    "test_sentences = test_df['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c22b1d",
   "metadata": {},
   "source": [
    "## show frequency of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fe51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([len(sentence) for sentence in train_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(sentence) for sentence in test_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d231dc",
   "metadata": {},
   "source": [
    "# tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7218e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(sentence) for sentence in np.concatenate([train_sentences, test_sentences], axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dcd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encode_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        attention_masks.append(encode_dict['attention_mask'])\n",
    "        \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks = encode_sentences(train_sentences, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd5bbb",
   "metadata": {},
   "source": [
    "# set dataset and dataloader for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d26bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a518c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.99)\n",
    "val_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              sampler=RandomSampler(train_dataset),\n",
    "                              batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            sampler=SequentialSampler(val_dataset),\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5b79c",
   "metadata": {},
   "source": [
    "# set pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3cc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_hidden: int, variance_epsilon: float = 1e-12):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(num_hidden))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_hidden))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d265340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBert(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        config.output_hidden_states = True\n",
    "        super(CustomBert, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = config.num_hidden_layers + 1\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        hidden_layers = outputs[2]\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer[:, 0, :]) for layer in hidden_layers], dim=2\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0) * cls_outputs).sum(-1)\n",
    "\n",
    "        # multisample dropout (wut): https://arxiv.org/abs/1905.09788\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [self.classifier(self.high_dropout(cls_output)) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        outputs = logits\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7db420",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CustomBert.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b25794",
   "metadata": {},
   "source": [
    "# set optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16932ec",
   "metadata": {},
   "source": [
    "# set learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "total_step = len(train_dataset) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bded19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(preds_flat == labels_flat) / len(preds_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ee8cd",
   "metadata": {},
   "source": [
    "# fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da3d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "creterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_val_loss = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_accuracy = 0.0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader, 1):\n",
    "        input_ids, attention_mask, labels = tuple(el.to(device) for el in batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(input_ids,\n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=attention_mask)\n",
    "        \n",
    "        loss = creterion(output, labels)\n",
    "        total_train_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    print(f'Train loss: {total_train_loss / len(train_dataloader)}')\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = tuple(el.to(device) for el in batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=attention_mask)\n",
    "        \n",
    "            loss = creterion(output, labels)\n",
    "            total_val_loss += loss\n",
    "            \n",
    "            \n",
    "        logits = output.detach().cpu().numpy()\n",
    "        label_ids = labels.detach().cpu().numpy()\n",
    "\n",
    "        total_val_accuracy += get_accuracy(logits, label_ids)\n",
    "        \n",
    "    val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f'Validation loss: {val_loss}')\n",
    "    print(f'Validation accuracy: {total_val_accuracy / len(val_dataloader)}')\n",
    "    \n",
    "    if epoch == 0:\n",
    "        current_val_loss = val_loss\n",
    "    else:\n",
    "        if current_val_loss <= val_loss:\n",
    "            print(f'Early stop: {epoch} epoch')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0da237",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00721606",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids, test_attention_masks = encode_sentences(test_sentences, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = torch.tensor(np.ones(len(test_input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b02f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids, attention_mask, labels = tuple(el.to(device) for el in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=None)\n",
    "        \n",
    "    logits = output.detach().cpu().numpy()\n",
    "    pred_flatten = np.argmax(logits, axis=1).flatten()\n",
    "    \n",
    "    predictions.extend(pred_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731856f",
   "metadata": {},
   "source": [
    "# calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd08de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4781d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_df[correct_df['target'].values == predictions].shape[0] / correct_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b074afd",
   "metadata": {},
   "source": [
    "# save submission file and submit predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b24378",
   "metadata": {},
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa92b96",
   "metadata": {},
   "source": [
    "with open('submission.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'target'])\n",
    "    for idx, target in zip(test_df['id'].values, predictions):\n",
    "        writer.writerow([idx, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba43ca4",
   "metadata": {},
   "source": [
    "#!kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"My third try\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f48cb",
   "metadata": {},
   "source": [
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d80f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478a637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c34153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644e696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7c19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640cb1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
